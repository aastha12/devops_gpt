###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> Gemini {\n  provider google-ai\n  retry_policy Exponential\n  options {\n    model \"gemini-2.0-flash-exp\"\n    api_key env.GOOGLE_API_KEY\n    generationConfig {\n      temperature 0.1\n      max_output_tokens 2048\n      top_p 0.8\n      top_k 20\n    }\n  }\n}\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.90.2\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "judge.baml": "// Defining a data model.\nclass JudgeEvaluation {\n  score int @description(\"A score from 1-5 based on how well the response follows instructions\")\n  justification string @description(\"Detailed reasoning for the score, explaining instruction understanding and requirements adherence\")\n}\n\n// Create a function to evaluate response quality\nfunction EvaluateResponse(prompt: string, response: string) -> JudgeEvaluation {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client Gemini // Set OPENAI_API_KEY to use this client.\n  //{{ ctx.output_format }} is replaced with a JSON schema of the function's return type(RootCauseAnalysis)\n  prompt #\"\n<evaluation_task>\n    <instruction>\n        You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\n        We will provide you with the user input and an AI-generated response.\n        You should first read the user input carefully to understand the task, and then evaluate the quality of the response based on the criteria provided in the <evaluation> section.\n        You will assign the response a rating following the <rating_rubric> and <evaluation_steps>. \n        Provide step-by-step explanations for your rating, and only choose ratings from the rubric.\n    </instruction>\n\n    <evaluation>\n        <metric_definition>\n            You will be assessing the model's ability to follow instructions provided in the user prompt.\n        </metric_definition>\n\n        <criteria>\n            <instruction_following>\n                The response demonstrates a clear understanding of the instructions in the user prompt, satisfying all of the instruction's requirements.\n            </instruction_following>\n        </criteria>\n    </evaluation>\n\n    <rating_rubric>\n        <rating score=\"5\">Complete fulfillment - Response addresses all aspects and adheres to all requirements of the instruction. The user would feel like their instruction was completely understood.</rating>\n        <rating score=\"4\">Good fulfillment - Response addresses most aspects and requirements of the instruction, with only minor omissions or deviations. The user would feel their instruction was well understood.</rating>\n        <rating score=\"3\">Some fulfillment - Response omits some minor aspects or ignores certain requirements. The user would feel their instruction was partially understood.</rating>\n        <rating score=\"2\">Poor fulfillment - Response addresses some aspects but misses key requirements. The user would feel their instruction was significantly misunderstood.</rating>\n        <rating score=\"1\">No fulfillment - Response fails to address core aspects of the instruction. The user would feel their request was not understood at all.</rating>\n    </rating_rubric>\n\n    <evaluation_steps>\n        <step1>Assess instruction understanding: Does the response address the intent of the instruction such that a user would not feel the instruction was ignored or misinterpreted?</step1>\n        <step2>Assess requirements adherence: Does the response meet specific requirements from the instruction such as format, tone, word count, or required content?</step2>\n    </evaluation_steps>\n\n    <input>\n        <user_prompt>{{prompt}}</user_prompt>\n        <ai_response>{{response}}</ai_response>\n    </input>\n\n    <output> \n        Please provide your evaluation in the following format:\n        {{ ctx.output_format}}\n    </output>\n</evaluation_task>\n  \"#\n}\n\n\n\n// Test the function with a sample evaluation\ntest sample_judge_evaluation {\n  functions [EvaluateResponse]\n  args {\n    prompt \"Analyze this Jenkins pipeline failure and provide troubleshooting steps\"\n    response \"The pipeline failed due to authentication issues. Here are some steps to fix it: 1. Check credentials 2. Verify permissions\"\n  }\n}",
    "root_cause.baml": "// Defining a data model.\nclass RootCauseAnalysis {\n  reasoning string @description(\"A step-by-step analysis of the patterns, common themes, and how the new incident relates.\")\n  root_cause_summary string @description(\"A concise summary of the most likely root causes.\")\n  troubleshooting_steps string[] @description(\"A numbered list of recommended troubleshooting steps in order of priority.\")\n}\n\n// Create a function to extract the RootCauseAnalysis from a string.\nfunction AnalyzeIncident(query: string, similar_incidents_str: string) -> RootCauseAnalysis {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client Gemini // Set OPENAI_API_KEY to use this client.\n  //{{ ctx.output_format }} is replaced with a JSON schema of the function's return type(RootCauseAnalysis)\n  prompt #\"\n  <task>\n   You are an expert SRE assistant. Given a new incident and a list of similar past incidents, analyze the\n  patterns to identify common root causes and suggest the next best troubleshooting steps.\n  </task>\n   \n  <new_incident>\n   {{ query }}\n  </new_incident>\n   \n  <similar_incidents>\n   {{ similar_incidents_str }}\n  </similar_incidents>\n   \n  Produce your final analysis in this exact format:\n   {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample RootCauseAnalysis. Open the VSCode playground to run this.\ntest jenkins_git_failure {\n  functions [AnalyzeIncident]\n  args {\n    query \"Jenkins pipeline failed with Git authentication error: 'remote: HTTP Basic: Access denied. The provided password or token is incorrect or your account has 2FA enabled and you must use a personal access token instead of a password.'\"\n    similar_incidents_str #\"\n    <incident>\n    Jenkins build failed - Git clone failed with authentication error. Pipeline was working yesterday but started failing today. Error: remote: HTTP Basic: Access denied\n    </incident>\n    \n    <incident>\n    CI/CD pipeline failing on Git checkout. Getting 403 Forbidden error when trying to access repository. Personal access token might be expired.\n    </incident>\n    \n    <incident>\n    Jenkins unable to pull from GitLab repository. Authentication failing with message about incorrect credentials. Token was regenerated last week.\n    </incident>\n    \"#\n  }\n}\n",
}

def get_baml_files():
    return file_map